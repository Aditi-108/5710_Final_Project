{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# **Starter code for BME 5710 project**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Import libraries and sub-libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "# You can install torchmeterics using: pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Calling a custom code to change the default font for figures to `Computer Modern`. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fontsetting import font_cmu\n",
    "# plt = font_cmu(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check the hardware that is at your disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Read training data from `data/train/hig-res` and `data/train/low-res`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 240\n"
     ]
    }
   ],
   "source": [
    "# Loading TIFF images for Super-resolution\n",
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None):\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted([f for f in os.listdir(high_res_dir) if f.endswith('.tif')])\n",
    "    \n",
    "    # Get the number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    # Get the sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        high_res_path = os.path.join(self.high_res_dir, self.filenames[idx])\n",
    "        low_res_path = os.path.join(self.low_res_dir, self.filenames[idx])\n",
    "\n",
    "        # Load images\n",
    "        high_res = Image.open(high_res_path)\n",
    "        low_res = Image.open(low_res_path)\n",
    "\n",
    "        # Resize low-res to 128x128 (ensuring correct input size)\n",
    "        low_res = low_res.resize((128, 128), Image.BICUBIC)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            high_res = self.transform(high_res)\n",
    "            low_res = self.transform(low_res)\n",
    "\n",
    "        return low_res, high_res  # Returning input-output pairs\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset for training images\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform)\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(dataset, batch_size):\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset size\n",
    "dataset_size = len(train_dataset)\n",
    "print('Number of images in the dataset:', dataset_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define a super-resolution network\n",
    "\n",
    "#### Here, I have defined a trivial network, which has only two layers and no activation function. We are essentially doing linear filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- TEMPLATE (TrivialNet) --------------------------------------------------------\n",
    "\n",
    "\n",
    "class TrivialNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrivialNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # First conv layer\n",
    "        x = self.conv2(x)  # Output layer\n",
    "        return x\n",
    "    \n",
    "# -------------- VERSION 1 --------------------------------------------------------\n",
    "\n",
    "# class UNet_V1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SuperResUNet, self).__init__()\n",
    "\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Conv2d(64, 128, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2)\n",
    "#         )\n",
    "\n",
    "#         self.middle = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, 256, 3, padding=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.output_layer = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.middle(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "\n",
    "# -------------- VERSION 2 ------------------------------------------------------------------------------\n",
    "\n",
    "# Architecture explanation:\n",
    "# The architecture is a U-Net with the following structure:\n",
    "# 1. Encoder: Two convolutional layers with ReLU activation and max pooling.\n",
    "# 2. Middle: Two convolutional layers with ReLU activation.\n",
    "# 3. Decoder: Three transposed convolutional layers with ReLU activation.\n",
    "# 4. Output: A single convolutional layer to produce the final output.\n",
    "\n",
    "# Features:\n",
    "# Encoder/Decoder (symmetricl downsampling and upsampling of Unet)\n",
    "    # Transposed Convolution (Upsampling)\n",
    "# Batch Normalization\n",
    "# Skip connections (Unet, retains spatial detail, e1 e2 encoder, d1 d2 decoder)\n",
    "\n",
    "# class UNet_V2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SuperResUNetV2, self).__init__()\n",
    "\n",
    "#         # Encoder block 1: convolution, batch normalization, ReLU, max pooling\n",
    "#         self.enc1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, 3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2)  # Downsample from 128x128 to 64x64\n",
    "#         )\n",
    "\n",
    "#         # Encoder block 2\n",
    "#         self.enc2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, 3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2)  # Downsample from 64x64 to 32x32\n",
    "#         )\n",
    "\n",
    "#         # Middle convolution block: deep feature extraction\n",
    "#         self.middle = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, 256, 3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Upsample from 32x32 to 64x64\n",
    "#         self.up1 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Decoder block 1: process concatenated feature maps (up1 + enc2)\n",
    "#         self.dec1 = nn.Sequential(\n",
    "#             nn.Conv2d(256, 128, 3, padding=1),  # 128 (up1) + 128 (enc2)\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Upsample from 64x64 to 128x128\n",
    "#         self.up2 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Decoder block 2: process concatenated feature maps (up2 + enc1)\n",
    "#         self.dec2 = nn.Sequential(\n",
    "#             nn.Conv2d(128, 64, 3, padding=1),  # 64 (up2) + 64 (enc1)\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Upsample from 128x128 to 256x256\n",
    "#         self.up3 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Final output layer to produce 1-channel (grayscale) output\n",
    "#         self.output_layer = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         e1 = self.enc1(x)         # Encoder output 1\n",
    "#         e2 = self.enc2(e1)        # Encoder output 2\n",
    "#         m = self.middle(e2)       # Bottleneck\n",
    "\n",
    "#         d1 = self.up1(m)          # Upsample from 32x32 to 64x64\n",
    "#         d1 = torch.cat((d1, e2), dim=1)  # Skip connection from encoder 2\n",
    "#         d1 = self.dec1(d1)\n",
    "\n",
    "#         d2 = self.up2(d1)         # Upsample from 64x64 to 128x128\n",
    "#         d2 = torch.cat((d2, e1), dim=1)  # Skip connection from encoder 1\n",
    "#         d2 = self.dec2(d2)\n",
    "\n",
    "#         d3 = self.up3(d2)         # Upsample from 128x128 to 256x256\n",
    "#         out = self.output_layer(d3)  # Final output\n",
    "#         return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a function to execute training. Note, we will call this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, criterion, train_loader, num_epoch):\n",
    "    avg_train_losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):  # Loop over the dataset multiple times\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, (x_tr_batch, y_tr_batch) in enumerate(train_loader):  # Loop over mini-batches\n",
    "            x_tr_batch, y_tr_batch = x_tr_batch.to(device), y_tr_batch.to(device)\n",
    "\n",
    "            # Upsample low-resolution input to 256x256\n",
    "            x_tr_batch = torch.nn.functional.interpolate(x_tr_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            opt.zero_grad()  # Delete previous gradients\n",
    "            y_hat_tr_batch = model(x_tr_batch)  # Forward pass\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            opt.step()  # Update weights\n",
    "            total_train_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)  # Compute average loss\n",
    "        avg_train_losses.append(avg_train_loss)  # Store average loss\n",
    "\n",
    "    # Plot training loss\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(range(1, num_epoch + 1), avg_train_losses, label='training loss')\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('MSE loss')\n",
    "    ax.set_yscale('log')  # Log scale for better visualization\n",
    "    ax.set_title('training loss')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Now, let us define hyperparameters and train the network. \n",
    "\n",
    "#### Note, in addition to the parameters that controls the network architecture or the training process, you need to select/initialize (i) a data loader, (ii) a model, (iii) an optimizer, and (iv) a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Number of complete images in each batch\n",
    "lr = 1e-3  # Learning rate\n",
    "num_epoch = 100  # Epochs\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "train_loader = create_loader(train_dataset, batch_size)\n",
    "model = TrivialNet().to(device)  # Pick a model and move to GPU/CPU\n",
    "# model = UNet_V1().to(device)  # Pick a model and move to GPU/CPU\n",
    "# model = UNet_V2().to(device)  # Pick a model and move to GPU/CPU\n",
    "opt = optim.Adam(model.parameters(), lr=lr)  # Pick an optimizer\n",
    "criterion = nn.MSELoss()  # Pick a loss function\n",
    "\n",
    "# Train the model\n",
    "train_model(model, opt, criterion, train_loader, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Apply it one of the validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset for validation images\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform)\n",
    "\n",
    "# Load one (low-res, high-res) image pair from validation dataset and move it to the dedvice\n",
    "val_low_res, val_high_res = val_dataset[1]  # Input (128x128), Ground truth (256x256)\n",
    "val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "# Upsample low-resolution image to 256x256 for both model input and error visualization\n",
    "val_low_res_interpolated = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False).squeeze(0)\n",
    "\n",
    "# Apply the trained model to super-resolve the interpolated low-res image\n",
    "val_super_res = model(val_low_res_interpolated.unsqueeze(0)).detach().squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "val_low_res_np = val_low_res_interpolated.squeeze().cpu().numpy()  # Use the interpolated version for error maps\n",
    "val_high_res_np = val_high_res.squeeze().cpu().numpy()\n",
    "val_super_res_np = val_super_res.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot an example image and error maps\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 7))\n",
    "\n",
    "# Plot images\n",
    "ax[0, 0].imshow(val_high_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 0].set_title('ground truth (high-res)')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(val_low_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 1].set_title('interpolated low-res image')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "ax[0, 2].imshow(val_super_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 2].set_title('super-resolved image')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "# Error maps\n",
    "ax[1, 0].imshow(5 * np.abs(val_high_res_np - val_high_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 0].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 1].imshow(5 * np.abs(val_high_res_np - val_low_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 1].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 2].imshow(5 * np.abs(val_high_res_np - val_super_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 2].axis('off')\n",
    "ax[1, 2].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 2].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PSNR and SSIM over the entire validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "# Accumulators\n",
    "total_psnr_interpolated = 0\n",
    "total_psnr_super_resolved = 0\n",
    "total_ssim_interpolated = 0\n",
    "total_ssim_super_resolved = 0\n",
    "num_samples = len(val_dataset)\n",
    "\n",
    "# Loop over validation set\n",
    "for i in range(num_samples):\n",
    "    val_low_res, val_high_res = val_dataset[i]\n",
    "    val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "    # Compute data range dynamically\n",
    "    data_range = val_high_res.max() - val_high_res.min()\n",
    "    ssim_metric.data_range = data_range\n",
    "\n",
    "    # Upsample low-res image\n",
    "    val_low_res_up = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False)\n",
    "    val_high_res = val_high_res.unsqueeze(0)  # Add batch dim\n",
    "    val_super_res = model(val_low_res_up).detach()\n",
    "\n",
    "    # PSNR\n",
    "    psnr_interp = psnr_metric(val_low_res_up, val_high_res).item()\n",
    "    psnr_sr = psnr_metric(val_super_res, val_high_res).item()\n",
    "\n",
    "    # SSIM (data_range is now set in metric initialization)\n",
    "    ssim_interp = ssim_metric(val_low_res_up, val_high_res).item()\n",
    "    ssim_sr = ssim_metric(val_super_res, val_high_res).item()\n",
    "\n",
    "    # Accumulate\n",
    "    total_psnr_interpolated += psnr_interp\n",
    "    total_psnr_super_resolved += psnr_sr\n",
    "    total_ssim_interpolated += ssim_interp\n",
    "    total_ssim_super_resolved += ssim_sr\n",
    "\n",
    "# Averages\n",
    "avg_psnr_interp = total_psnr_interpolated / num_samples\n",
    "avg_psnr_sr = total_psnr_super_resolved / num_samples\n",
    "avg_ssim_interp = total_ssim_interpolated / num_samples\n",
    "avg_ssim_sr = total_ssim_super_resolved / num_samples\n",
    "\n",
    "# Print results\n",
    "print(f'Average PSNR (interpolated): {avg_psnr_interp:.2f} dB')\n",
    "print(f'Average PSNR (super-resolved): {avg_psnr_sr:.2f} dB')\n",
    "print(f'Average SSIM (interpolated): {avg_ssim_interp:.4f}')\n",
    "print(f'Average SSIM (super-resolved): {avg_ssim_sr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
